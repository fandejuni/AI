\documentclass[journal, a4paper]{IEEEtran}

\usepackage{graphicx}   
%\usepackage{subfigure}
\usepackage{url}        
\usepackage{amsmath}    
% Some useful/example abbreviations for writing math
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\ypred}{\mathbf{\hat y}}

\begin{document}

% Define document title, do NOT write author names
\title{Agents in a Multiplayer Snake Environment}
\author{Anonymous Authors}
\maketitle

% Write abstract here
\begin{abstract}
	A short summary of your project. You should change also the title, but do \emph{not} enter any author names or anything that unnecessarily identifies any of the authors. It is suggested you use a similar structure (sections, etc.) as demonstrated in this document, but you can make the section headings more descriptive if you wish. Of course \emph{you should delete all the text in this template and write your own}! -- this text simply provides detailed instructions/hints on how to proceed.

\end{abstract}

% Each section begins with a \section{title} command
\section{Introduction}

Describe what you did. Provide access to your anonymized code\footnote{Our code is available here: \url{http://anonymouslinktoyourcode.zip}}.

Note that results should be reproducible using the technologies from the labs (i.e., Python, and selecting among Scikit-Learn, OpenAI Gym, TensorFlow, PyGame, \ldots).

Do not change the formatting (columns, margins, etc). Hint: shared tools like \texttt{http://sharelatex.com/} and \texttt{http://overleaf.com/} are great tools for collaborating on a multi-author report in latex. If you wish to use Word, base it on the IEEE template\footnote{\url{https://www.ieee.org/publications_standards/publications/conferences/2014_04_msw_a4_format.doc}} and convert to \texttt{pdf} for submission. 

\section{Background and Related Work}

\subsection{Reinforcement Learning}


\subsubsection{Learning}

\textbf{Q-learning algorithm}
\cite{lecture-rl}
\cite{lecture-rl2}
\cite{intro-rl}
\cite{qlearning}
\[
	Q^{t+1}(s_t, a_t) = (1-\alpha) Q^t(s_t, a_t) + \alpha (r_t + \gamma \max\limits_b Q^t(s_{t+1}, b))
\]

\textbf{SARSA algorithm}
\cite{lecture-rl}
\cite{lecture-rl2}
\cite{intro-rl}
\cite{sarsa}
\[
	Q^{t+1}(s_t, a_t) = (1-\alpha) Q^t(s_t, a_t) + \alpha (r_t + \gamma Q^t(s_{t+1}, a_{t+1}))
\]

\subsubsection{Exploitation vs. Exploration}

\textbf{$\epsilon$-greedy exploration}
\cite{lecture-rl}
\cite{lecture-rl2}
\cite{intro-rl}
With probability $\epsilon$, the agent chooses a random action to explore and learn the consequences.

\textbf{Selection with softmax operator}
\cite{intro-rl}
\[
    P\left(a_t = a\right) = \frac{\exp\left(Q^t(s_t, a) / \tau\right)}{\sum\limits_b \exp\left(Q^t(s_t, b)\right) / \tau}
\]


Elaborate (in your own words) the background material required to understand your work. It should cover a subset of the topics touched upon in the course. You are encouraged to cite topics in lectures, e.g., structured output prediction in , book chapters, e.g., Chapter 9 from , or articles from the literature, e.g., . Basically, you should prepare the reader to understand what you are about to present in the following sections. Eq.~\eqref{eq:MAP} shows a random equation.
\begin{equation}
	\label{eq:MAP}
	% Note the example \newcommand s defined above which make it faster to write latex math
	\ypred = \argmax_{\y \in \{0,1\}} p(\y|\x)
\end{equation}

\section{The Environment}

Describe your environment, either one you adapted/borrowed from somewhere, or designed yourself. Convince the reader that it is an interesting and/or challenging environment (could it potentially have real-world use or is based on real-world data? Or simply to provide an interesting/fun/challenging problem to tackle. In particular you should outline the particular challenges it poses as a RL problem.

\section{The Agent}

We designed two different agents for this environment to compare different strategies.
The \emph{Reinforcement Learning} (\emph{RL}) agent is model-free, whereas the \emph{Minimax} agent is based on the exploration of the different possibilities.

\subsection{The Reinforcement Learning Agent\label{rl_agent}}

% TODO: cite papers
This model-free agent uses either SARSA or Q-Learning to learn the model and the consequences of its actions.
It also uses either $\epsilon$-greedy exploration or the Softmax method to tackle the dilemma between exploitation and exploration.

To perceive its environment without having too many states to learn, we use $12$ inputs.
When the input is bounded, if the real value is greater than this or doesn't exist, it is assigned the greatest value.
\begin{enumerate}
    \item $0$ if the head of the snake touches the right wall, $1$ otherwise.
    \item $0$ if the head of the snake touches the upper wall, $1$ otherwise.
    \item $0$ if the head of the snake touches the left wall, $1$ otherwise.
    \item $0$ if the head of the snake touches the lower wall, $1$ otherwise.
    \item The distance between the square located to the right of the head and the closest candy (between $0$ and $7$).
    \item The distance between the square located to the top of the head and the closest candy (between $0$ and $7$).
    \item The distance between the square located to the left of the head and the closest candy (between $0$ and $7$).
    \item The distance between the square located to the bottom of the head and the closest candy (between $0$ and $7$).
    \item The distance between the square located to the right of the head and the closest square of another snake (between $0$ and $3$).
    \item The distance between the square located to the top of the head and the closest square of another snake (between $0$ and $3$).
    \item The distance between the square located to the left of the head and the closest square of another snake (between $0$ and $3$).
    \item The distance between the square located to the bottom of the head and the closest square of another snake (between $0$ and $3$).
\end{enumerate}
These inputs allows the snake to have a local vision of the candies, a local but smaller vision of the other snakes, and feel when it touches a wall.

We also give rewards after each action done by the snake.
\begin{itemize}
    \item $1$ if the snake ate a candy.
    \item $-10$ if the snake died.
    \item $0$ otherwise.
\end{itemize}

The agent will easily learn to look for candies and to avoid walls.
It will also learn to avoid other snakes.

Finally, we use the following parameters:
\begin{itemize}
    \item $\epsilon = 0.1$ in the case of $\epsilon$-greedy exploration.
    \item $\tau = 0.1$ in the case of Softmax exploration.
    \item $\alpha = 0.2$. A learning rate so high is strong enough because the inputs and the best actions to take are highly correlated.
    \item $\gamma = 0.9$. We don't use $1$ to make it clear that the best path is always the shortest, but it is near $1$ since we care about the future almost as much as the present.
\end{itemize}

\subsection{The Minimax Agent}

\section{Results and Discussion}

To measure how well the two agents perform in our environment, we use several measurements.
We introduce the \emph{Random} agent, which chooses at every step a random direction.
We simulate the games on a grid of size $30$, with $10$ candies on the map and the two adversarial agents we want to compare.

\subsection{Performance of your Agent in your Environment}

\subsubsection{Performance of Learning}

\begin{figure}[h]
	\centering
    \includegraphics[width=0.95\columnwidth]{images/learning_curve_against_random.pdf}
    \caption{\label{learning_curve_against_random}The learning curve of the RL agent against a random one.}
\end{figure}

To analyze the learning time of the \emph{RL} agent, we simulate it against the \emph{Random} agent.
At the beginning, the \emph{RL} agent knows nothing.
We run $5000$ games, and we compute the frequency of victories for the \emph{RL} agent for every chunk of $100$ games.
The results are presented on Figure~\ref{learning_curve_against_random}.
As we can see, with only a few hundreds games, this agent is far better than the \emph{Random} one.

\subsubsection{Performance of the agents}

We simulated $1000$ games between each pair of agents.
The results are presented in Table~\ref{comparative_table}.
When the sum of the scores isn't equal to $1000$, it means that games ended because both agents died at the same time.

As can be seen, the \emph{Minimax} agent is nearly unbeatable, no agent scored a point against it.
However, it scored $933$ against the \emph{Random} agent, which means that it was killed $77$ (while killing the other one).
Even though the \emph{RL} dies $4$ times out of $1000$ without killing the \emph{Random} agent, it performs better with respect to its own score.
In $991$ out of $1000$ games, it manages to stay alive while killing its opponent, which means it learned from experience to avoid going too close to its opponent, whereas the $emph{Minimax}$ agent thinks its opponent will take the best action.
This hypothesis is clearly false for the \emph{Random} agent.

\begin{table}[h]
	\caption{\label{comparative_table}Results of the games.}
	\centering
	\begin{tabular}{llll}
		\hline
        \textbf{Game} & \textbf{Random} & \textbf{Minimax}  \\
		\hline
        \textbf{RL} & 991 - 4 & 0 - 174 \\
        \textbf{Minimax} & 933 - 0 &  \\
		\hline
	\end{tabular}
\end{table}

\begin{figure}[h]
	\centering
    \includegraphics[width=0.95\columnwidth]{images/histo_size.pdf}
    \caption{\label{histo_size}Distribution of the proportion of the size of the \emph{RL} agent for the $776$ games ended after $1000$ iterations.}
\end{figure}

The game between the \emph{RL} and \emph{Minimax} agents ends before $1000$ iterations $174$ times out of $1000$.
For $50$ of theses games, it ends in a tie, where both agents are killed at the same iteration.
The other $776$ games are interesting to look at.
When a game reaches the maximum number of iterations, we stop it, and measure the sizes of the two agents.
The distribution of the proportion of the size of the \emph{RL} agent compared to the sum of both sizes is presented on Figure~\ref{histo_size}.
The main difference between the \emph{RL} agent and the \emph{Minimax} one is the local vision of the former, which can only ``see'' candies and snakes up to a Manhattan distance of $8$ and $4$ squares.
The average propotion size of $25$ \% for the \emph{RL} agent is thus really strong.

\subsection{Performance of your Agent in the ALife Environment}

We only tried to deploy our \emph{RL} agent in the ALife Environment since it is model-free.
Deploying the \emph{Minimax} agent would have require to create a totally new model, which would have made it a different agent.
When using the same technics (Q-Learning or SARSA, $\epsilon$-greedy or Softmax exploration), the results aren't as satisfactory as in the Snake world.
This environment is much more complicated, and the inputs and outputs are analogous.

We chose to tackle these issues by discretizing the inputs.
We don't use the energy input.
We multiply each other input (the color sensors) by $5$, and round them to the lower integer.
To keep the number of actions small, we only allow rotations of $-\frac{\pi}{2}$, $0$, $\frac{\pi}{2}$ and $pi$.

After a few minutes of simulation, it seems that the herbivore have understood that they have to eat the plants.
Unfortunately, this timeframe doesn't seem sufficient enough for the insects to learn other lessons.

\section{Conclusion and Future Work}
    This environment is interesting to use for multiple agents, because it has a few aspects.
    A snake has to eat candies to grow, but in the same time it also has to avoid collisions with walls and other snakes.
    The inputs defined in Subsection~\cite{rl_agent} allows an agent to have a local but precise vision of its environment.
    Such an agent learns quickly and is able to outperform a \emph{Random} agent after a few hundreds games.
    When competing against a strong AI, the \emph{Minimax} agent, whose only limitation is the depth it can explore when deciding which action to take, its results are quite satisfactory.

    The main issue with the strategy we chose is the high number of states.
    For instance, the agent has to learn many times that going into a wall is bad, because there are a lot of different states where it is touching a wall.
    To continue this work, one should focus on Deep Reinforcement Learning, to give the snake a bigger vision of its environment, and make it easier and quicker for it to learn the best strategy.

\begin{thebibliography}{4}

	\bibitem{lecture-rl} % Web document
	N.~Tziortziotis. Lecture IV - Introduction to Reinforcement Learning. \textit{INF581 Advanced Topics in Artificial Intelligence}, 2018.

	\bibitem{lecture-rl2} % Web document
	N.~Tziortziotis. Lecture V, part II - Approximate and Bayesian Reinforcement Learning. \textit{INF581 Advanced Topics in Artificial Intelligence}, 2018.

	\bibitem{qlearning}
	Watkins, Christopher \& Dayan, Peter. (1992). Technical Note: Q-Learning. Machine Learning. 8. 279-292. 10.1007/BF00992698. 

	\bibitem{sarsa}
	A. Rummery, G \& Niranjan, Mahesan. (1994). On-Line Q-Learning Using Connectionist Systems. Technical Report CUED/F-INFENG/TR 166. 

    \bibitem{intro-rl}
    Sutton, R. S. \& Barto, A. G. 1998 Reinforcement learning: an introduction. Cambridge, MA: MIT Press.

\end{thebibliography}

\end{document}
