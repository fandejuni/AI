\documentclass[journal, a4paper]{IEEEtran}

\usepackage{graphicx}   
%\usepackage{subfigure}
\usepackage{url}        
\usepackage{amsmath}    
% Some useful/example abbreviations for writing math
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\ypred}{\mathbf{\hat y}}

\begin{document}

% Define document title, do NOT write author names
\title{The Title of your Report}
\author{Anonymous Authors}
\maketitle

% Write abstract here
\begin{abstract}
	A short summary of your project. You should change also the title, but do \emph{not} enter any author names or anything that unnecessarily identifies any of the authors. It is suggested you use a similar structure (sections, etc.) as demonstrated in this document, but you can make the section headings more descriptive if you wish. Of course \emph{you should delete all the text in this template and write your own}! -- this text simply provides detailed instructions/hints on how to proceed.

\end{abstract}

% Each section begins with a \section{title} command
\section{Introduction}

Describe what you did. Provide access to your anonymized code\footnote{Our code is available here: \url{http://anonymouslinktoyourcode.zip}}.

Note that results should be reproducible using the technologies from the labs (i.e., Python, and selecting among Scikit-Learn, OpenAI Gym, TensorFlow, PyGame, \ldots).

Do not change the formatting (columns, margins, etc). Hint: shared tools like \texttt{http://sharelatex.com/} and \texttt{http://overleaf.com/} are great tools for collaborating on a multi-author report in latex. If you wish to use Word, base it on the IEEE template\footnote{\url{https://www.ieee.org/publications_standards/publications/conferences/2014_04_msw_a4_format.doc}} and convert to \texttt{pdf} for submission. 

\section{Background and Related Work}

Elaborate (in your own words) the background material required to understand your work. It should cover a subset of the topics touched upon in the course. You are encouraged to cite topics in lectures, e.g., structured output prediction in \cite{LectureSOP}, book chapters, e.g., Chapter 9 from \cite{Barber}, or articles from the literature, e.g., \cite{Astar,DeepMindSC2}. Basically, you should prepare the reader to understand what you are about to present in the following sections. Eq.~\eqref{eq:MAP} shows a random equation.
\begin{equation}
	\label{eq:MAP}
	% Note the example \newcommand s defined above which make it faster to write latex math
	\ypred = \argmax_{\y \in \{0,1\}} p(\y|\x)
\end{equation}

\section{The Environment}

Describe your environment, either one you adapted/borrowed from somewhere, or designed yourself. Convince the reader that it is an interesting and/or challenging environment (could it potentially have real-world use or is based on real-world data? Or simply to provide an interesting/fun/challenging problem to tackle. In particular you should outline the particular challenges it poses as a RL problem.

\section{The Agent}

We designed two different agents for this environment to compare different strategies.
The \emph{Reinforcement Learning} (\emph{RL}) agent is model-free, whereas the \emph{Minimax} agent is based on the exploration of the different possibilities.

\subsection{The Reinforcement Learning Agent}

% TODO: cite papers
This model-free agent uses either SARSA or Q-Learning to learn the model and the consequences of its actions.
It also uses either $\epsilon$-greedy exploration or the Softmax method to tackle the dilemma between exploitation and exploration.

To perceive its environment without having too many states to learn, we use $12$ inputs.
\begin{enumerate}
    \item $0$ if the head of the snake touches the right wall, $1$ otherwise.
    \item $0$ if the head of the snake touches the upper wall, $1$ otherwise.
    \item $0$ if the head of the snake touches the left wall, $1$ otherwise.
    \item $0$ if the head of the snake touches the lower wall, $1$ otherwise.
    \item The distance between the square located to the right of the head and the closest candy (if greater than $7$, this input is $7$).
    \item The distance between the square located to the top of the head and the closest candy (if greater than $7$, this input is $7$).
    \item The distance between the square located to the left of the head and the closest candy (if greater than $7$, this input is $7$).
    \item The distance between the square located to the bottom of the head and the closest candy (if greater than $7$, this input is $7$).
    \item The distance between the square located to the right of the head and the closest square of another snake (if greater than $3$, this input is $3$).
    \item The distance between the square located to the top of the head and the closest square of another snake (if greater than $3$, this input is $3$).
    \item The distance between the square located to the left of the head and the closest square of another snake (if greater than $3$, this input is $3$).
    \item The distance between the square located to the bottom of the head and the closest square of another snake (if greater than $3$, this input is $3$).
\end{enumerate}
These inputs allows the snake to have a local vision of the candies, a local but smaller vision of the other snakes, and feel when it touches a wall.

We also give rewards after each action done by the snake.
\begin{itemize}
    \item $1$ if the snake ate a candy.
    \item $-10$ if the snake died.
    \item $0$ otherwise.
\end{itemize}

The agent will easily learn to look for candies and to avoid walls.
It will also learn to avoid other snakes.

Finally, we use the following parameters:
\begin{itemize}
    \item $\epsilon = 0.1$ in the case of $\epsilon$-greedy exploration.
    \item $\tau = 0.1$ in the case of Softmax exploration.
    \item $\alpha = 0.2$. A learning rate so high is strong enough because the inputs and the best actions to take are highly correlated.
    \item $\gamma = 0.9$. We don't use $1$ to make it clear that the best path is always the shortest, but it is near $1$ since we care about the future almost as much as the present.
\end{itemize}

\subsection{The Minimax Agent}

\section{Results and Discussion}

To measure how well the two agents perform in our environment, we use several measurements.
We introduce the \emph{Random} agent, which chooses at every step a random direction.
We simulate the matches on a grid of size $30$, with $10$ candies on the map and the two adversarial agents we want to compare.

\subsection{Performance of your Agent in your Environment}

\subsubsection{Performance of Learning}

\begin{figure}[h]
	\centering
    \includegraphics[width=0.8\columnwidth]{images/learning_curve_against_random.pdf}
    \caption{\label{learning_curve_against_random}The learning curve of the RL agent against a random one.}
\end{figure}

To analyze the learning time of the \emph{RL} agent, we simulate it against the \emph{Random} agent.
At the beginning, the \emph{RL} agent knows nothing.
We run $5000$ matches, and we compute the frequency of victories for the \emph{RL} agent for every chunk of $100$ matches.
The results are presented on Figure~\ref{learning_curve_against_random}.
As we can see, with only a few hundreds matches, this agent is far better than the \emph{Random} one.

\subsubsection{Performance of the agents}

\begin{table}[h]
	\caption{\label{comparative_table}Results of the matches.}
	\centering
	\begin{tabular}{llll}
		\hline
        \textbf{Match} & \textbf{Minimax} & \textbf{Random} \\
		\hline
        \textbf{RL} & 0 - 0 & 4744 - 113 \\
        \textbf{Minimax} & 0 - 0 & - \\
		\hline
	\end{tabular}
\end{table}

\subsection{Performance of your Agent in the ALife Environment}

We only tried to deploy our \emph{RL} agent in the ALife Environment since it is model-free.
Deploying the \emph{Minimax} agent would have require to create a totally new model, which would have made it a different agent.
When using the same technics (Q-Learning or SARSA, $\epsilon$-greedy or Softmax exploration), the results aren't as satisfactory as in the Snake world.
This environment is much more complicated, and the inputs and outputs are analogous.

We chose to tackle these issues by discretizing the inputs.
We don't use the energy input.
We multiply each other input (the color sensors) by $5$, and round them to the lower integer.
To keep the number of actions small, we only allow rotations of $-\frac{\pi}{2}$, $0$, $\frac{\pi}{2}$ and $pi$.

After a few minutes of simulation, it seems that the herbivore have understood that they have to eat the plants.
Unfortunately, this timeframe doesn't seem sufficient enough for the insects to learn other lessons.

\section{Conclusion and Future Work}
	This section summarizes the paper: Your environment and agent, its strength and its weaknesses. Also remark about what would be the next steps you would take if you or someone else were to continue/extend this project. 
	Note that for the initial submission you are limited strictly to 4 pages (double column), \emph{not including references}. An extra page will be allowed for final submission (after the initial reviews). 

% The bibliography:
\begin{thebibliography}{4}

	\bibitem{Barber} % Book
	D.~Barber. Bayesian Reasoning and Machine Learning,
	{\em Cambridge University Press}, 2012.

	\bibitem{LectureSOP} % Web document
		J.~Read. Lecture III - Structured Output Prediction and Search. \textit{INF581 Advanced Topics in Artificial Intelligence}, 2018.

	\bibitem{Astar}
	D.~Mena et al. A family of admissible heuristics for A* to perform inference in probabilistic classifier chains.
	{\em Machine Learning}, vol. 106, no. 1, pp 143-169, 2017.

	\bibitem{DeepMindSC2}
	O.~Vinyals et al. StarCraft {II:} {A} New Challenge for Reinforcement Learning.
	\url{https://arxiv.org/abs/1708.04782}, 2017. 

\end{thebibliography}

% Your document ends here!
\end{document}
